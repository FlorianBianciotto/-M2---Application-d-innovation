{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load wor2vec of train\n",
    "# train_word2vec_filename = 'word2vec/' + 'train_review_word2vec.csv'\n",
    "# train_word2vec_df = pd.read_csv(train_word2vec_filename)\n",
    "# train_word2vec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train = pd.read_csv(\"train_clean_stem.csv\", sep=';',index_col=0)\n",
    "max_sen_len = train.commentaire_stemmed.map(len).max()\n",
    "train = train[:10000]\n",
    "# train, X_test = train_test_split(train, train_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(train, padding=True, sg=1, min_count=1, size=500, workers=3, window=3):\n",
    "    if  padding:\n",
    "        print(len(train))\n",
    "        temp_df = pd.Series(train['commentaire_stemmed']).values\n",
    "        temp_df = list(temp_df)\n",
    "        for i in range(min_count):\n",
    "            temp_df.append(['pad'])\n",
    "        word2vec_file = 'model/' + 'word2vec_' + str(size) + '_PAD.model'\n",
    "    else:\n",
    "        temp_df = train['commentaire_stemmed']\n",
    "        word2vec_file = 'model/' + 'word2vec_' + str(size) + '.model'\n",
    "    w2v_model = Word2Vec(temp_df, min_count = min_count, size = size, workers = workers, window = window, sg = sg)\n",
    "\n",
    "    w2v_model.save(word2vec_file)\n",
    "    return w2v_model, word2vec_file\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel, word2vec_file = make_word2vec_model(train, padding=True, sg=sg, min_count=min_count, size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_file = 'model/' + 'word2vec_'+str(size)+'_PAD.model'\n",
    "w2vmodel = Word2Vec.load(word2vec_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_idx = w2vmodel.wv.vocab['pad'].index\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2vmodel.wv.vocab:\n",
    "            padded_X[i] = 0\n",
    "#             print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.vocab[word].index\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the output tensor\n",
    "def make_target(label):\n",
    "    return torch.tensor([label], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_sizes=(1,2,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = size\n",
    "NUM_FILTERS = 10\n",
    "import gensim\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        w2vmodel = gensim.models.KeyedVectors.load(word2vec_file)\n",
    "        weights = w2vmodel.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors), padding_idx=w2vmodel.wv.vocab['pad'].index)\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        print(EMBEDDING_SIZE)\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # [B, T, E]\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval(pred,Y):\n",
    "    true = 0\n",
    "    false = 0\n",
    "    \n",
    "    dtrue = {}\n",
    "    dfalse = {}\n",
    "    \n",
    "    for i in range (len(pred)):\n",
    "        if((pred[i]+1)/2 == float(Y[i].replace(',','.'))):\n",
    "            true += 1\n",
    "            if float(Y[i].replace(',','.')) not in dtrue:\n",
    "                dtrue[float(Y[i].replace(',','.'))] = 0\n",
    "            else:\n",
    "                dtrue[float(Y[i].replace(',','.'))] += 1\n",
    "        else:\n",
    "            false += 1\n",
    "            if float(Y[i].replace(',','.')) not in dfalse:\n",
    "                dfalse[float(Y[i].replace(',','.'))] = 0\n",
    "            else:\n",
    "                dfalse[float(Y[i].replace(',','.'))] += 1\n",
    "        if i % 20000 == 0:\n",
    "            print('Eval',i)\n",
    "            \n",
    "    print(dtrue)\n",
    "    print(dfalse)\n",
    "    try:\n",
    "        dratio = {dtrue[i] / (dtrue[i] + dfalse[i]) for i in dtrue}\n",
    "    except:\n",
    "        dratio = {}\n",
    "    return (true / (true + false)), dratio\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(corp,cnn):\n",
    "    start_time_predict = time.time()\n",
    "    bow_cnn_predictions = []\n",
    "    original_lables_cnn_bow = []\n",
    "    cnn.eval()\n",
    "    with torch.no_grad():\n",
    "        for index, row in corp.iterrows():\n",
    "            if index % 10000 == 0:\n",
    "                print('Prediction index',index)\n",
    "            bow_vec = make_word2vec_vector_cnn(row['commentaire_stemmed'])\n",
    "            probs = cnn(bow_vec)\n",
    "            _, predicted = torch.max(probs.data, 1)\n",
    "            bow_cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "    print(\"Time taken to predict: \" + str(time.time() - start_time_predict))\n",
    "    \n",
    "    print(bow_cnn_predictions)\n",
    "    \n",
    "    return eval(bow_cnn_predictions,corp['note'])\n",
    "    \n",
    "    # loss_file_name = 'plots/' + 'cnn_class_big_loss_with_padding.csv'\n",
    "    # loss_df = pd.read_csv(loss_file_name)\n",
    "    # print(loss_df.columns)\n",
    "    # plt_200_treduce_padding_3_epochs = loss_df[' loss'].plot()\n",
    "    # fig = plt_200_treduce_padding_3_epochs.get_figure()\n",
    "    # fig.savefig('plots/' + 'plt_200_treduce_padding_3_epochs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_csv(\"dev_clean_stem.csv\", sep=';',index_col=0)\n",
    "dev = dev[:10000]\n",
    "#dev, X_dev = train_test_split(dev, train_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Epoch1\n",
      "Time taken for iterow 0 : 0.0009777545928955078\n",
      "Time taken for iterow 1000 : 14.210352182388306\n",
      "Time taken for iterow 2000 : 14.056207418441772\n",
      "Time taken for iterow 3000 : 15.087676286697388\n",
      "Time taken for iterow 4000 : 14.00814962387085\n",
      "Time taken for iterow 5000 : 13.786942720413208\n",
      "Time taken for iterow 6000 : 14.023356914520264\n",
      "Time taken for iterow 7000 : 14.30992603302002\n",
      "Time taken for iterow 8000 : 14.078436136245728\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 10\n",
    "VOCAB_SIZE = len(w2vmodel.wv.vocab)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "# loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(cnn_model.parameters(), lr=0.001, momentum=0.9)\n",
    "num_epochs = 30\n",
    "\n",
    "# Open the file for writing loss\n",
    "loss_file_name = 'plots/' + 'cnn_class_big_loss_with_padding.csv'\n",
    "f = open(loss_file_name,'w')\n",
    "f.write('iter, loss')\n",
    "f.write('\\n')\n",
    "losses = []\n",
    "best_dev_accuracy = 0\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch\" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    start_time_epoch = time.time()\n",
    "    start_time_iterow = time.time()\n",
    "    for index, row in train.iterrows():\n",
    "        \n",
    "        if index % 1000 == 0:\n",
    "            print(\"Time taken for iterow \"+str(index)+\" : \" + str(time.time() - start_time_iterow))\n",
    "            start_time_iterow = time.time()\n",
    "       \n",
    "        \n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word2vec_vector_cnn(row['commentaire_stemmed'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = make_target(int((float(train['note'][index].replace(',','.'))*2)-1))\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Time taken for epoch \"+str(epoch)+\" : \" + str(time.time() - start_time_epoch)) \n",
    "    \n",
    "    pred_dev, dratio = pred(dev,cnn_model)\n",
    "    print('Prediction on dev',pred_dev)\n",
    "    print(dratio)\n",
    "    if pred_dev > best_dev_accuracy:\n",
    "        best_dev_accuracy = pred_dev\n",
    "        torch.save(cnn_model, 'model/' + 'cnn_model_200_wp_'+str(epoch)+'.pth')\n",
    "        \n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch ran :\"+ str(epoch+1))\n",
    "    f.write(str((epoch+1)) + \",\" + str(train_loss / len(train)))\n",
    "    f.write('\\n')\n",
    "    train_loss = 0\n",
    "    torch.save(cnn_model, 'model/' + 'cnn_model_200_wp.pth')\n",
    "\n",
    "\n",
    "f.close()\n",
    "print(\"Input vector\")\n",
    "print(bow_vec.cpu().numpy())\n",
    "print(\"Probs\")\n",
    "print(probs)\n",
    "print(torch.argmax(probs, dim=1).cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Save test prediction\n",
    "# f = open(\"result/results_svc_word2vec.txt\", \"a\")\n",
    "# for i in range(len(test_predictions_word2vec)):\n",
    "#     f.write(test.iloc[i]['review_id'] + \" \" + str(test_predictions_word2vec[i]) + \"\\n\")\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour 5000 data pour size 200  \n",
    "Equilibrer fenetre 1,2 ?spe (50000 data) 14%  \n",
    "Non Equilibre fenetre 1,2 36spe          19%  \n",
    "NE F3,5 spe  14%   \n",
    "\n",
    "size 200 meilleur temps d'apprentissage ????  \n",
    "size 300 ne f1,2 40spe 12%  \n",
    "size 100 ne f1,2 60spe 18%  \n",
    "size 200 ne f1,2,3 120spe 17% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
